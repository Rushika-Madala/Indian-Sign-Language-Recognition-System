{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34b7f-5cff-472b-a0a7-7d7abe679279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1f4de-a37a-4824-a4e5-c828ab1a1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eb09c-b473-401a-a499-863e90ae7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10683c8a-f2da-443d-976c-2aee5b311fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting mediapipe holistics\n",
    "#we are creating 2 variables ,one for mediapipe holistic and second one for mediapipe drawing utilities\n",
    "#mediapipe holistoc and mediapipe drawing utilites are 2 components in the mediapipe \n",
    "#mediapipe holistic helps in pose estimation  whereas the mediapipe drawing utilities helps in visualizing the results for the developers\n",
    "mp_holistic =mp.solutions.holistic #imports the holistic module from the solutions which is a sub module in mediapipe\n",
    "mp_drawing=mp.solutions.drawing_utils #imports the drawing module from the solutions which is a sub module in mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc18ff-4e97-4a22-968e-1750c502678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially when we get the feed it is in the form of BGR(blue,green,red),detection are made using mediapipe .mediapipe works with images in\n",
    "#RGB(red,green,blue) format.so we make transformations using opencv.The opencv works with the images in BGR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39348752-287d-4d94-8e0d-0ab3054d472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):   #setting a function for detection.here model is mediapipe holistic model\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)  #converts image from BGR 2 RGB color space.cvtcolor() is function that changes imag\n",
    "    image.flags.writeable=False   #sets image unwriteable so that the mediapipe won't the image data during processing\n",
    "    results=model.process(image)  #process() is a function . It processes the image using the holistic model\n",
    "    image.flags.writeable=True    #sets back image to writeable after processing\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)  #converts image from RGB to BGR color space\n",
    "    return image,results          #return the processed image,detection results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8579a-8d62-48b5-a40e-0e90f26f4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fd710-e17c-455b-a6e4-4c9f527a8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image,results):#styling the landmarks like size,colors etc\n",
    "    # Draw face connections and styling face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections  and styling pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius=4),\n",
    "                               mp_drawing.DrawingSpec(color=(80,44,121),thickness=2,circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections  and styling left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76),thickness=2,circle_radius=4),\n",
    "                               mp_drawing.DrawingSpec(color=(121,44,250),thickness=2,circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  and styling right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d042b-b4e6-4e10-9caa-e32833849fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =cv2.VideoCapture(0)  #accesing webcam.default index of webcam is 0,storing everything got from video in cap avriable\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():                   #checking whether the webcam is open or not.so used isOpened() function \n",
    "        ret,frame=cap.read()                #reading the feed.while reading, we are getting 2 values they are return value(true/false) ,image i.e frame\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "        print(results)\n",
    "        draw_styled_landmarks(image,results)        #draw landmarks\n",
    "        cv2.imshow('opencv feed',image)     #showing what has read  the user.\"opencv feed\" is name given to frame\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):#breaking the loop.  if the current key pressed is 'q' the loop breaks\n",
    "            break                                                           \n",
    "    cap.release()                           #releases the webcam\n",
    "    cv2.destroyAllWindows()                 #closes all opencv windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7b2f3-253a-490c-aa75-b6a7bb4cdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a1455-67b0-4b44-a859-c75ae9d37f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67ded1-6a0f-4575-adb3-ba2306f9f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ecced-c94a-4f12-a1e8-3e42630c42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array is used to create arrays\n",
    "#pose_landmarks has x,y,z,visibility rest has only x,y,z\n",
    "#results.pose_landmarks give the result in form of landmark{x:,y:,z:,visibility:} \n",
    "#result.pose_lanmarks.landmark give the result in form of list i.e [x: y: z: visibility:, etc]\n",
    "#using flatten converts into 1-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3651b7-b92f-41dc-b16f-7be01acaf523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):   # function to extract key points \n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    lh=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,face,lh,rh]) #cancatenating arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee6bba-fd34-43ca-be32-634736734d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c5515-6ef7-425c-ac4e-2a8ac0299a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03318eb2-a0d8-4664-90c4-4bf8df279746",
   "metadata": {},
   "outputs": [],
   "source": [
    "468*3+21*3+21*3+33*4==len(extract_keypoints(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca60803-63b1-4283-8650-5c65e830036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00d2fe-9226-42a2-8288-338f04c8d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks.landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d641c09-0749-4d19-a239-dd1cbec9445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bd3c9-4d03-4533-a4ed-4c893914304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.face_landmarks.landmark),len(results.face_landmarks.landmark)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfd27b-740d-427a-8b65-c03a4d45cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.pose_landmarks.landmark),len(results.pose_landmarks.landmark)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9778f-85f2-49e5-94b1-089e0c35fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.left_hand_landmarks.landmark),len(results.left_hand_landmarks.landmark)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948015d0-4d48-4444-a943-5bb517428fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b3e40-419a-4c2c-95c7-afb9dca104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=os.path.join('MP_DATA') #DATA_PATH is path where exported data is stored i.e numpy arrays,here the exported data is extracted keypoints\n",
    "actions=np.array(['hello','thanks','iloveyou']) #detecting the actions hello ,thanks,iloveyou\n",
    "no_sequences=30                                 #using 30 frames do detect the action,collecting 30 videos for each action\n",
    "sequence_length=30                              #each video is of length 30 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99298cf7-01d9-4586-b52f-c46289205edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MP_DATA is directory\n",
    "#action detection means takes sequence of data to predict the action rather than single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b94f9-7656-4a24-87ee-35bd2b314a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating folders to store the data\n",
    "for action in actions:  #it iterates over every action\n",
    "    for sequence in range(no_sequences): #it iterates over range of numbers from o to 29(no_sequences-1)\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))  #it is creating directory for each combination of action and sequence number \n",
    "        except:                                                        #using os.makedirs()\n",
    "            pass                                                        #if directory exists it goes to except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129d1d4-212b-4185-992b-4ba1a8d90955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5 collecting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d793ee7-02aa-4e2b-b9e4-e5e9e76c50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =cv2.VideoCapture(0) \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                    ret,frame=cap.read()     #each frame is read from video feed          \n",
    "                    image,results=mediapipe_detection(frame,holistic) #pose detection is performed on the image using the func and return the landmarks\n",
    "                    print(results)   #on the image and pose detection results\n",
    "                    draw_styled_landmarks(image,results)   #landmarks are styled on each image    \n",
    "                    cv2.imshow('opencv feed',image)        #displaying the image\n",
    "                    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "                        break    \n",
    "                    if frame_num == 0: \n",
    "                             #message displayed started data collection\n",
    "                             cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                             cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                             # Show to screen\n",
    "                             cv2.imshow('OpenCV Feed', image)\n",
    "                             cv2.waitKey(2000)\n",
    "                    else:\n",
    "                  \n",
    "                             #displays the ongoing data collection\n",
    "                             cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                             # Show to screen\n",
    "                             cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints) #keypoints extraced from pose detection are saved in numpy arrays and stored in corresponding directory \n",
    "                                                #of action,sequence number,frame\n",
    "    cap.release()        #releases video object                   \n",
    "    cv2.destroyAllWindows()      #open cv windows are destroyed           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ba004-78bc-4512-be2a-d146f1cda92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection break: having breaks between each sequence collection allows you to reset and reposition yourself to collect the action from start to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96ea1-3792-4624-9b16-4c260fd24f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6 preprocess data create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c9749-f3a4-447a-877a-06997cb2925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # train_test_split : splits the dataset into training and testing\n",
    "from tensorflow.keras.utils import to_categorical # to_categorical : conerts integer into categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92a334-af90-46a1-bddf-19e5f7ea1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a label array that represents actions\n",
    "label_map={label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6c789-23b4-48a3-b8c1-68390dba9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d2e72-e061-425c-b035-f52d579196cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels=[] ,[] #sequences represents x-data ,labels represents y-data\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window=[]\n",
    "        for frame_num in range(sequence_length):\n",
    "            res=np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b823dc0-26db-4f93-8b93-487f79614665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9597e9-11aa-45ed-8892-973d788fdc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6240641-f2d4-4f1b-836f-263bdb0704e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocesing\n",
    "X=np.array(sequences)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89150b57-58b6-4617-8c2a-8be8be14b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54309f-92b2-47fe-9b5b-84e4a0eae43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int) #binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f4b14-3414-4c8d-a607-32a10daa18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d51c3b-f0f6-423d-8744-749bb3f6d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb3575-ee49-46cd-b9b2-fb79b27f3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape ,y_train.shape,y_test.shape#training and testing tuples size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94787f45-dc51-41c4-8295-3e4346f329fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step7 training lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db11b85-e584-4303-9746-8e74ea5058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequential helps to build neural networks layer by layer in a sequential manner.layers are added to model one by one \n",
    "#and each is connected to previous layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "#LSTM is a rnn layer which helps in handling the sequential data\n",
    "#Dense is a fully connected neural network layer.each neuron is connected to each neuron in previous layer ,helps in learning the complex patterns\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "#TensoBoard is a visualization tool for monitoring and visualizing various aspects like model training and evaluation\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98382a-4ba9-47f9-a84a-2e2f55eaca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')#creates the directory path named logs\n",
    "tb_callback = TensorBoard(log_dir=log_dir) # tb_callback is a tensorboard callback object.tensorboard writes the log files at path log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a67fd-3c80-495c-8a83-c8286857740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "#A sequential model architecture consists of multiple LSTM layers followed by dense layers.the activation function 'relu' is\n",
    "#used in LSTM and dense layers except last dense layer that uses softmax activation function.softmax is used for multi class classification\n",
    "#the model is designed to take input sequence 30 with 1662 features\n",
    "#return_sequence =false means final output should be returned ,true means it should return sequence of outputs not only final output \n",
    "#64,128,32 means neurons in the layer\n",
    "#activation functions decide whether the neuron should be activated or not.they are applied on output of each neuron\n",
    "#Relu =max(0,x)\n",
    "#softmax used in output layer in multi classification models to produce probabilites of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d1e89-8c5b-45ec-b269-1fdff7a1801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential() #sequential model\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,1662))) \n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(actions.shape[0],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f02a2-baae-4223-b57c-883bd4964bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape[0] #output layer has 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb16554-9a7a-40eb-bcf2-60b13c18ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy']) #model compiling\n",
    "#optimizer:updates the weigths of neural network during training,Adam is an algorithm\n",
    "#loss function:it finds the difference between actual and predicted output.categorical_crossentropy is a loss function used for multiclassclassification\n",
    "#metrics:used to evaluate the performance of model.metric used is categorical_accuracy.it is used in multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf2f1b-3a48-4d4e-9f11-e3a6d9245cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=100,callbacks=[tb_callback])\n",
    "#epochs:The number of times the entire training dataset will be passed forward and backward through the neural network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad19b4-1bc5-494d-bc2b-c63aa366a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a0de5-3483-4e7e-b8c5-25f4ef5d8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527eef4b-104d-4966-af6d-2d66b5d80035",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efde17e-306e-4f39-ab9b-7b114dba15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e263245-b6be-4d57-921e-9622f61edb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e609ef-c355-47de-8706-e322ba128f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97779f7-9cd0-43bb-a321-c941d06db35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13deb841-e3a0-496b-afb9-7e02ee176642",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcfef9-1807-4c68-9af0-e300f728c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c442e-6da4-4c1c-9571-920886821cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae70eb-3a27-4fb9-80b6-bd2554eca83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d8b64-2659-41c6-8d5c-667bdf389afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0a282-ee28-4b61-8daa-b6367bc17744",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf75d7-f195-4071-b8ec-49a6519593f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a69d79-5f62-4c3f-b8bd-9d517252bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0560c-e495-4c9f-99a4-7fbac1eb1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 11 test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb90d2a-db68-4028-ba99-22dd0fe34a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5cc22-695f-4b3e-bd80-d5f3663da005",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0897887-ac44-4cee-b13e-836bf598e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375489f-66bf-4709-b8ce-99bde16d9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.append('def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1517c68-077e-47db-9622-9d1aba7d4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac91d69-3f49-47c8-992c-2aacaa20126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396cda8-4225-4079-9bfe-7194dc6ebef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.4\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "     \n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "           #sequence.insert(0,keypoints)\n",
    "          #sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "         \n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f37c6a-befe-4b91-9076-8c2a262711ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
